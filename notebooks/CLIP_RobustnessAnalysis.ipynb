{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#               Testing the model             #\n",
    "###############################################\n",
    "\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List\n",
    "from clip.simple_tokenizer import SimpleTokenizer\n",
    "import torchvision.transforms as T\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clip_tokenizer = SimpleTokenizer()\n",
    "\n",
    "#############################################\n",
    "#                Dataset Class              #\n",
    "#############################################\n",
    "\n",
    "class SegmentationDatasetWithText(Dataset):\n",
    "    def __init__(self, root_dir, transform_img, transform_label=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform_img = transform_img\n",
    "        self.transform_label = transform_label\n",
    "\n",
    "        # Assume images are in `root_dir/color` and masks are in `root_dir/masks`\n",
    "        self.image_dir = os.path.join(root_dir, 'color')\n",
    "        self.mask_dir = os.path.join(root_dir, 'label')\n",
    "\n",
    "        # Collect all image filenames\n",
    "        self.image_paths = sorted([\n",
    "            fname for fname in os.listdir(self.image_dir)\n",
    "            if fname.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_filename = self.image_paths[index]\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        mask_path = os.path.join(self.mask_dir, image_filename.replace('.jpg', '.png'))\n",
    "\n",
    "        # Load the image and apply image transform.\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform_img:\n",
    "            image = self.transform_img(image)\n",
    "\n",
    "        # Load the mask, convert to numpy, then to tensor.\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        mask = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "\n",
    "        # If a label transform is provided, first unsqueeze to add a channel dimension.\n",
    "        if self.transform_label:\n",
    "            if mask.ndim == 2:  # mask shape [H, W]\n",
    "                mask = mask.unsqueeze(0)  # now shape [1, H, W]\n",
    "            mask = self.transform_label(mask)\n",
    "            mask = mask.squeeze(0)  # back to shape [H, W]\n",
    "\n",
    "        # Generate a text prompt.\n",
    "        name, _ = os.path.splitext(image_filename)\n",
    "        match = re.match(r\"^(cat|dog)_([A-Za-z_]+)_(\\d+)\", name)\n",
    "        if match:\n",
    "            animal_type = match.group(1)           # 'cat' or 'dog'\n",
    "            breed = match.group(2).replace(\"_\", \" \") # Replace underscores with spaces.\n",
    "            # Optionally, you can title-case the breed:\n",
    "            breed = breed.title()\n",
    "            text_prompt = f\"a photo of a {breed} {animal_type}\"\n",
    "        else:\n",
    "            # Provide a default text prompt if the regex does not match\n",
    "            print(f\"Warning: Filename {image_filename} does not match expected format.\")\n",
    "            text_prompt = \"a photo of an animal\"\n",
    "\n",
    "        token_ids = clip.tokenize([text_prompt]).squeeze(0)\n",
    "\n",
    "        decoded_text = clip_tokenizer.decode(token_ids.tolist())\n",
    "        #print(f\"Original: {text_prompt} → Tokenized: {decoded_text}\")\n",
    "\n",
    "        return image, mask, token_ids\n",
    "\n",
    "class EncodeMask:\n",
    "    def __call__(self, mask):\n",
    "        # Do not convert to float; work with the mask as is (which should be Long)\n",
    "        mask = torch.where(mask < 36, torch.tensor(0, dtype=mask.dtype, device=mask.device), mask)\n",
    "        mask = torch.where((mask >= 36) & (mask < 192), torch.tensor(1, dtype=mask.dtype, device=mask.device), mask)\n",
    "        mask = torch.where(mask >= 192, torch.tensor(2, dtype=mask.dtype, device=mask.device), mask)\n",
    "        mask = torch.clamp(mask, min=0, max=2)\n",
    "        # print(\"Mask dtype:\", mask.dtype)  # Should be torch.int64\n",
    "        # print(\"Mask unique values:\", torch.unique(mask))  # Should only contain [0, 1, 2]\n",
    "        return mask  # Already Long\n",
    "\n",
    "def denormalize(image_tensor):\n",
    "    \"\"\"Convert a normalized tensor image back to a standard (0,1) range for visualization.\"\"\"\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
    "    image_tensor = image_tensor * std + mean  # Undo normalization\n",
    "    image_tensor = torch.clamp(image_tensor, 0, 1)  # Ensure values are valid\n",
    "    return image_tensor\n",
    "\n",
    "#############################################\n",
    "#             Segmentation Head             #\n",
    "#############################################\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class ImprovedCLIPSegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels, text_dim, num_classes, dropout_prob=0.25, use_attention=True):\n",
    "        super(ImprovedCLIPSegmentationHead, self).__init__()\n",
    "        print(f'text_dim: {text_dim}')\n",
    "        print(f'in_channels: {in_channels}')\n",
    "        self.text_proj = nn.Linear(text_dim, in_channels)\n",
    "        self.fuse_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(dropout_prob)\n",
    "        )\n",
    "        self.residual_block = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        if use_attention:\n",
    "            self.attention = ChannelAttention(256)\n",
    "        else:\n",
    "            self.attention = nn.Identity()\n",
    "        self.fuse_conv2 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, img_features, text_features):\n",
    "        B, C, H, W = img_features.shape  # should be [B, 768, H, W]\n",
    "\n",
    "        # Ensure both inputs are float32 to avoid mismatched precision issues\n",
    "        img_features = img_features.float()\n",
    "        text_features = text_features.float()\n",
    "\n",
    "        projected_text = self.text_proj(text_features)  # => [B, 768]\n",
    "        projected_text = projected_text.view(B, C, 1, 1)  # => [B, 768, 1, 1]\n",
    "        projected_text = projected_text.expand(B, C, H, W)  # => [B, 768, H, W]\n",
    "        fused = torch.cat([img_features, projected_text], dim=1)\n",
    "        x = self.fuse_conv1(fused)\n",
    "        res = self.residual_block(x)\n",
    "        x = self.relu(x + res)\n",
    "        x = self.attention(x)\n",
    "        logits = self.fuse_conv2(x)\n",
    "        return logits\n",
    "\n",
    "#############################################\n",
    "#          CLIP Segmentation Model          #\n",
    "#############################################\n",
    "\n",
    "class CLIPSegmentationModel(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=3):\n",
    "        super(CLIPSegmentationModel, self).__init__()\n",
    "        # CLIP ViT-L/14 usually outputs 768-dim features\n",
    "        self.feature_dim = clip_model.visual.output_dim  # should be 768\n",
    "        print(f'feature dim: {self.feature_dim}')        # prints 768\n",
    "\n",
    "        self.clip_model = clip_model\n",
    "\n",
    "        # Both in_channels and text_dim are 768\n",
    "        self.seg_head = ImprovedCLIPSegmentationHead(\n",
    "            in_channels=768,  # matches self.feature_dim\n",
    "            text_dim=768,     # text encoder dimension\n",
    "            num_classes=num_classes,\n",
    "            dropout_prob=0.25\n",
    "        )\n",
    "\n",
    "    def get_visual_features(self, image):\n",
    "        visual = self.clip_model.visual\n",
    "        # Ensure input image has the same dtype as model weights (e.g., fp16)\n",
    "        image = image.to(dtype=visual.conv1.weight.dtype)\n",
    "        # Step 1: Convolution.\n",
    "        x = visual.conv1(image)  # [B, width, H', W']\n",
    "        # Flatten the spatial dimensions.\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # [B, width, tokens]\n",
    "        x = x.permute(0, 2, 1)  # [B, tokens, width]\n",
    "        # Step 2: Prepend the class token.\n",
    "        cls_tokens = visual.class_embedding.to(x.dtype) + torch.zeros(\n",
    "            x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device\n",
    "        )\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # [B, tokens+1, width]\n",
    "        # Step 3: Add positional embedding.\n",
    "        x = x + visual.positional_embedding.to(x.dtype)\n",
    "        # Step 4: Layer norm pre-transformer.\n",
    "        x = visual.ln_pre(x)\n",
    "        # Step 5: Run the transformer.\n",
    "        x = x.permute(1, 0, 2)  # [tokens+1, B, width]\n",
    "        x = visual.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # [B, tokens+1, transformer_width]\n",
    "        # **Apply projection (if available) to get final visual features.**\n",
    "        if hasattr(visual, \"proj\"):\n",
    "            x = x @ visual.proj  # now x is [B, tokens+1, output_dim] (e.g. [B, tokens+1, 768])\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, token_ids):\n",
    "        # Convert input images to float32\n",
    "        image = image.float()\n",
    "\n",
    "        visual_features = self.get_visual_features(image)  # [B, num_tokens+1, 768]\n",
    "\n",
    "        tokens = visual_features[:, 1:, :]  # remove CLS token => [B, num_tokens, 768]\n",
    "        B, N, D = tokens.shape  # D should be 768\n",
    "        grid_size = int(np.sqrt(N))\n",
    "\n",
    "        image_features = tokens.reshape(B, grid_size, grid_size, 768).permute(0, 3, 1, 2)  # [B, 768, H, W]\n",
    "\n",
    "        # Convert text features to float32 before passing\n",
    "        text_features = self.clip_model.encode_text(token_ids).float()  # Ensure float32\n",
    "\n",
    "        seg_logits = self.seg_head(image_features, text_features)\n",
    "\n",
    "        seg_logits = F.interpolate(seg_logits, size=(image.shape[2], image.shape[3]),\n",
    "                                  mode=\"bilinear\", align_corners=False)\n",
    "        return seg_logits\n",
    "\n",
    "###############################################\n",
    "# Training, Evaluation, and Utility Functions #\n",
    "###############################################\n",
    "def compute_iou_per_class(preds, targets, num_classes=3):\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union (IoU) for each class.\n",
    "    Returns a dictionary with IoU for each class.\n",
    "    \"\"\"\n",
    "    iou_per_class = {}\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (preds == cls)\n",
    "        target_inds = (targets == cls)\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = (pred_inds | target_inds).sum().item()\n",
    "\n",
    "        if union == 0:\n",
    "            iou_per_class[cls] = float('nan')  # Avoid division by zero\n",
    "        else:\n",
    "            iou_per_class[cls] = intersection / union\n",
    "\n",
    "    return iou_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import random_noise\n",
    "\n",
    "def add_salt_and_pepper_noise(image, amount):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    amount: float value representing the proportion of pixels to be replaced with noise.\n",
    "            For example: 0.00, 0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.14, 0.16, 0.18.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) with salt and pepper noise added.\n",
    "    \"\"\"\n",
    "    # Convert image to float in range [0,1] for skimage\n",
    "    image_float = image.astype(np.float32) / 255.0\n",
    "    noisy = random_noise(image_float, mode='s&p', amount=amount)\n",
    "    # Convert back to [0,255] uint8\n",
    "    noisy = np.clip(noisy * 255.0, 0, 255).astype(np.uint8)\n",
    "    return noisy\n",
    "\n",
    "def occlude_image(image, square_edge):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    square_edge: integer, the edge length of the square to occlude.\n",
    "                 For example: 0, 5, 10, 15, 20, 25, 30, 35, 40, 45.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) with a randomly placed square region occluded.\n",
    "    \"\"\"\n",
    "    perturbed = image.copy()\n",
    "    if square_edge > 0:\n",
    "        h, w, _ = perturbed.shape\n",
    "        # Ensure the square fits within the image\n",
    "        max_x = w - square_edge\n",
    "        max_y = h - square_edge\n",
    "        if max_x < 0 or max_y < 0:\n",
    "            # If the square is larger than the image, occlude the whole image\n",
    "            perturbed[:] = 0\n",
    "        else:\n",
    "            # Randomly select the top-left corner for occlusion.\n",
    "            x = np.random.randint(0, max_x + 1)\n",
    "            y = np.random.randint(0, max_y + 1)\n",
    "            perturbed[y:y+square_edge, x:x+square_edge, :] = 0\n",
    "    return perturbed\n",
    "\n",
    "def decrease_brightness(image, offset):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    offset: integer offset to subtract from each pixel. For example: 0, 5, 10, 15, 20, 25, 30, 35, 40, 45.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) with decreased brightness.\n",
    "    \"\"\"\n",
    "    # Convert to a type that supports negative values.\n",
    "    perturbed = image.astype(np.int32) - offset\n",
    "    # Clip values so that they do not fall below 0.\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def increase_brightness(image, offset):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    offset: integer offset to add to each pixel. For example: 0, 5, 10, 15, 20, 25, 30, 35, 40, 45.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) with increased brightness.\n",
    "    \"\"\"\n",
    "    # Convert to an integer type that can hold values > 255\n",
    "    perturbed = image.astype(np.int32) + offset\n",
    "    # Clip values to the valid range and convert back to uint8\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def decrease_contrast(image, factor):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    factor: multiplicative factor for pixel values.\n",
    "            For example: 1.0, 0.95, 0.90, 0.85, 0.80, 0.60, 0.40, 0.30, 0.20, 0.10.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) with decreased contrast.\n",
    "    \"\"\"\n",
    "    perturbed = image.astype(np.float32) * factor\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def increase_contrast(image, factor):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    factor: multiplicative factor for pixel values.\n",
    "            For example: 1.0, 1.01, 1.02, 1.03, 1.04, 1.05, 1.1, 1.15, 1.2, 1.25.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) with increased contrast.\n",
    "    \"\"\"\n",
    "    perturbed = image.astype(np.float32) * factor\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def add_gaussian_noise(image, sigma):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    sigma: standard deviation of the Gaussian noise.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255)\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, sigma, image.shape)\n",
    "    perturbed = image.astype(np.float32) + noise\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def add_gaussian_blur(image, iterations):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    iterations: integer, the number of times to convolve the image with a 3x3 Gaussian kernel.\n",
    "                Use 0 for no blurring, 1 for a single pass, up to 9 for a heavy blur.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) that has been blurred.\n",
    "    \"\"\"\n",
    "    import cv2  # OpenCV is used for the convolution.\n",
    "    # Define a 3x3 Gaussian kernel.\n",
    "    kernel = (1/16) * np.array([[1, 2, 1],\n",
    "                                [2, 4, 2],\n",
    "                                [1, 2, 1]], dtype=np.float32)\n",
    "    \n",
    "    perturbed = image.copy()\n",
    "    for _ in range(iterations):\n",
    "        perturbed = cv2.filter2D(perturbed, ddepth=-1, kernel=kernel)\n",
    "    return perturbed\n",
    "\n",
    "\n",
    "def dice_score_multiclass(pred, target, num_classes=3, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the mean Dice score over num_classes.\n",
    "    pred and target should be 2D tensors of shape [H, W] containing class indices.\n",
    "    \"\"\"\n",
    "    dice_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls).float()\n",
    "        target_cls = (target == cls).float()\n",
    "        intersection = (pred_cls * target_cls).sum()\n",
    "        union = pred_cls.sum() + target_cls.sum()\n",
    "        dice_cls = (2 * intersection + epsilon) / (union + epsilon)\n",
    "        dice_per_class.append(dice_cls.item())\n",
    "    return np.mean(dice_per_class)\n",
    "\n",
    "def test_best_model_on_perturbations(best_model_path, test_dataset, batch_size=1, num_classes=3, perturbation_mode='gaussian_noise'):\n",
    "    \"\"\"\n",
    "    Evaluates the best segmentation model on perturbed test data.\n",
    "    \n",
    "    For perturbation_mode 'gaussian_noise', the function will:\n",
    "      - Loop over 10 sigma levels: {0, 2, 4, …, 18}.\n",
    "      - For each sigma level, it denormalizes the test image to [0,255], applies Gaussian noise,\n",
    "        clips the result to [0,255], re-normalizes using CLIP's mean/std, and then runs the model.\n",
    "      \n",
    "    For perturbation_mode 'gaussian_blur', it:\n",
    "      - Loops over 10 iteration levels: {0, 1, 2, …, 9}.\n",
    "      - For each level, it applies the 3×3 Gaussian kernel that many times.\n",
    "    \n",
    "    In both cases, the function computes the mean multi-class Dice score over the test set,\n",
    "    plots Dice score vs. perturbation level, saves the figure, and shows a few example perturbed images.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Load CLIP model and set to evaluation.\n",
    "    clip_model, _ = clip.load(\"ViT-L/14\", device=device)\n",
    "    clip_model.eval()\n",
    "    for param in clip_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Load segmentation model.\n",
    "    model = CLIPSegmentationModel(clip_model=clip_model, num_classes=num_classes).to(device)\n",
    "    state_dict = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    # Define CLIP normalization parameters.\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
    "\n",
    "    # Set up perturbation parameters.\n",
    "    if perturbation_mode == 'gaussian_noise':\n",
    "        levels = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
    "        perturb_fn = add_gaussian_noise\n",
    "    elif perturbation_mode == 'gaussian_blur':\n",
    "        levels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        perturb_fn = add_gaussian_blur\n",
    "    elif perturbation_mode == 'contrast_increase':\n",
    "        levels = [1.0, 1.01, 1.02, 1.03, 1.04, 1.05, 1.1, 1.15, 1.2, 1.25]\n",
    "        perturb_fn = increase_contrast\n",
    "    elif perturbation_mode == 'contrast_decrease':\n",
    "        levels = [1.0, 0.95, 0.90, 0.85, 0.80, 0.60, 0.40, 0.30, 0.20, 0.10]\n",
    "        perturb_fn = decrease_contrast\n",
    "    elif perturbation_mode == 'brightness_increase':\n",
    "        levels = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "        perturb_fn = increase_brightness\n",
    "    elif perturbation_mode == 'brightness_decrease':\n",
    "        levels = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "        perturb_fn = decrease_brightness\n",
    "    elif perturbation_mode == 'occlusion_increase':\n",
    "        levels = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "        perturb_fn = occlude_image\n",
    "    elif perturbation_mode == 'salt_and_pepper_noise':\n",
    "        levels = [0.00, 0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.14, 0.16, 0.18]\n",
    "        perturb_fn = add_salt_and_pepper_noise\n",
    "    else:\n",
    "        raise ValueError(f\"Perturbation mode '{perturbation_mode}' not implemented.\")\n",
    "\n",
    "    dice_scores_per_level = []\n",
    "\n",
    "    # Loop over each perturbation level.\n",
    "    for level in levels:\n",
    "        level_dice_scores = []\n",
    "        for images, masks, token_ids in test_loader:\n",
    "            # Denormalize the image to get pixel values in [0,1], then scale to [0,255].\n",
    "            images_denorm = denormalize(images)\n",
    "            images_pixels = (images_denorm * 255.0).clamp(0, 255).cpu().numpy().astype(np.uint8)\n",
    "            img_np = np.transpose(images_pixels[0], (1, 2, 0))\n",
    "\n",
    "            # Apply the selected perturbation.\n",
    "            perturbed_np = perturb_fn(img_np, level)\n",
    "\n",
    "            # Convert the perturbed image back to a tensor.\n",
    "            perturbed_tensor = torch.from_numpy(perturbed_np).permute(2, 0, 1).unsqueeze(0).float()\n",
    "            perturbed_tensor = perturbed_tensor / 255.0\n",
    "            perturbed_tensor = (perturbed_tensor - mean) / std\n",
    "            perturbed_tensor = perturbed_tensor.to(device)\n",
    "\n",
    "            token_ids = token_ids.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(perturbed_tensor, token_ids)\n",
    "            # Get predicted class mask.\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze(0)\n",
    "\n",
    "            # Compute multi-class Dice score.\n",
    "            dice = dice_score_multiclass(pred_mask, masks, num_classes=num_classes)\n",
    "            level_dice_scores.append(dice)\n",
    "        mean_dice = np.mean(level_dice_scores)\n",
    "        dice_scores_per_level.append(mean_dice)\n",
    "        print(f\"Perturbation Level {level} ({perturbation_mode}): Mean Dice Score = {mean_dice:.4f}\")\n",
    "\n",
    "    # Plot the mean Dice score vs. perturbation level.\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(levels, dice_scores_per_level, marker='o')\n",
    "    plt.xlabel(f\"{perturbation_mode} level\")\n",
    "    plt.ylabel(\"Mean Dice Score\")\n",
    "    plt.title(f\"Robustness Evaluation: Dice Score vs {perturbation_mode}\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{perturbation_mode}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    '''# Display example perturbations for a few selected levels (e.g., low, mid, high).\n",
    "    example_levels = [levels[0], levels[len(levels)//2], levels[-1]]\n",
    "    example_image, _, _ = next(iter(test_loader))\n",
    "    example_denorm = denormalize(example_image) * 255.0  # [0,255] values\n",
    "    example_np = np.transpose(example_denorm[0].cpu().numpy().astype(np.uint8), (1, 2, 0))\n",
    "\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, lev in enumerate(example_levels):\n",
    "        if perturbation_mode == 'gaussian_noise':\n",
    "            perturbed_example = perturb_fn(example_np, lev)\n",
    "        elif perturbation_mode == 'gaussian_blur':\n",
    "            perturbed_example = perturb_fn(example_np, lev)\n",
    "        plt.subplot(1, len(example_levels), i+1)\n",
    "        plt.imshow(perturbed_example)\n",
    "        plt.title(f\"Level {lev}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(f\"Examples of {perturbation_mode} perturbations\")\n",
    "    plt.show()'''\n",
    "\n",
    "    return dice_scores_per_level\n",
    "\n",
    "# Example usage in __main__:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load CLIP preprocessing transform.\n",
    "    clip_model, clip_preprocess = clip.load(\"ViT-L/14\", device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    test_transform = clip_preprocess  # Use the same image preprocessing as for training.\n",
    "\n",
    "    # Define a label transform that resizes the mask and encodes its values properly.\n",
    "    transform_label = T.Compose([\n",
    "        T.Resize((224, 224), interpolation=T.InterpolationMode.NEAREST),\n",
    "        EncodeMask()\n",
    "    ])\n",
    "\n",
    "    test_dataset = SegmentationDatasetWithText(\n",
    "        root_dir=\"./processed/Test/\",\n",
    "        transform_img=test_transform,\n",
    "        transform_label=transform_label\n",
    "    )\n",
    "\n",
    "    best_model_path = \"./clip_OPENAI_segmentation_best.pth\"\n",
    "\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='gaussian_noise')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='gaussian_blur')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='contrast_increase')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='contrast_decrease')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='brightness_increase')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='brightness_decrease')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='occlusion_increase')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='salt_and_pepper_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipemag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
