{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the CLIP features model classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#               Testing the model             #\n",
    "###############################################\n",
    "\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List\n",
    "from clip.simple_tokenizer import SimpleTokenizer\n",
    "import torchvision.transforms as T\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clip_tokenizer = SimpleTokenizer()\n",
    "\n",
    "#############################################\n",
    "#                Dataset Class              #\n",
    "#############################################\n",
    "\n",
    "import os\n",
    "import re\n",
    "import clip\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from clip.simple_tokenizer import SimpleTokenizer\n",
    "\n",
    "clip_tokenizer = SimpleTokenizer()\n",
    "\n",
    "class SegmentationDatasetWithText(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for segmentation with text prompts.\n",
    "    \n",
    "    Assumes:\n",
    "      - Images are in root_dir/color.\n",
    "      - Masks are in root_dir/label.\n",
    "      - Each mask is a color-coded image where:\n",
    "          • Background: Black [0,0,0] → 0\n",
    "          • Cat: Orange [255,165,0] → 1\n",
    "          • Dog: Cyan [0,255,255] → 2\n",
    "      - A text prompt is generated from the filename, e.g.:\n",
    "            \"cat_Siamese_27.png\" → \"a photo of a Siamese cat\"\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform_img, transform_label=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform_img = transform_img\n",
    "        self.transform_label = transform_label\n",
    "        \n",
    "        self.image_dir = os.path.join(root_dir, 'color')\n",
    "        self.mask_dir = os.path.join(root_dir, 'label')\n",
    "        \n",
    "        self.image_paths = sorted([\n",
    "            fname for fname in os.listdir(self.image_dir)\n",
    "            if fname.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_filename = self.image_paths[index]\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        # Use the same stem and force a .png extension for the mask.\n",
    "        mask_path = os.path.join(self.mask_dir, Path(image_filename).stem + \".png\")\n",
    "        \n",
    "        # Load and transform the image.\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform_img:\n",
    "            image = self.transform_img(image)\n",
    "        \n",
    "        # Load the mask as RGB and map colors to class indices.\n",
    "        mask_img = Image.open(mask_path).convert(\"RGB\")\n",
    "        mask_np = np.array(mask_img, dtype=np.uint8)\n",
    "        label_new = np.zeros((mask_np.shape[0], mask_np.shape[1]), dtype=np.int64)\n",
    "        \n",
    "        # Map background: black → 0.\n",
    "        background_mask = (mask_np[..., 0] == 0) & (mask_np[..., 1] == 0) & (mask_np[..., 2] == 0)\n",
    "        label_new[background_mask] = 0\n",
    "        \n",
    "        # Map cat: orange [255,165,0] → 1.\n",
    "        cat_mask = (mask_np[..., 0] == 255) & (mask_np[..., 1] == 165) & (mask_np[..., 2] == 0)\n",
    "        label_new[cat_mask] = 1\n",
    "        \n",
    "        # Map dog: cyan [0,255,255] → 2.\n",
    "        dog_mask = (mask_np[..., 0] == 0) & (mask_np[..., 1] == 255) & (mask_np[..., 2] == 255)\n",
    "        label_new[dog_mask] = 2\n",
    "        \n",
    "        mask = torch.from_numpy(label_new).long()\n",
    "        if self.transform_label:\n",
    "            if mask.ndim == 2:\n",
    "                mask = mask.unsqueeze(0)\n",
    "            mask = self.transform_label(mask)\n",
    "            mask = mask.squeeze(0)\n",
    "        \n",
    "        # Generate text prompt based on filename.\n",
    "        name, _ = os.path.splitext(image_filename)\n",
    "        match = re.match(r\"^(cat|dog)_([A-Za-z_]+)_(\\d+)\", name)\n",
    "        if match:\n",
    "            animal_type = match.group(1)  # \"cat\" or \"dog\"\n",
    "            breed = match.group(2).replace(\"_\", \" \")\n",
    "            breed = breed.title()\n",
    "            text_prompt = f\"a photo of a {breed} {animal_type}\"\n",
    "        else:\n",
    "            text_prompt = \"a photo of an animal\"\n",
    "        \n",
    "        token_ids = clip.tokenize([text_prompt]).squeeze(0)\n",
    "        \n",
    "        # (Optional) Decode the tokens for debugging.\n",
    "        decoded_text = clip_tokenizer.decode(token_ids.tolist())\n",
    "        # print(f\"Original: {text_prompt} → Tokenized: {decoded_text}\")\n",
    "        \n",
    "        return image, mask, token_ids\n",
    "\n",
    "def denormalize(image_tensor):\n",
    "    \"\"\"Convert a normalized tensor image back to a standard (0,1) range for visualization.\"\"\"\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
    "    image_tensor = image_tensor * std + mean  # Undo normalization\n",
    "    image_tensor = torch.clamp(image_tensor, 0, 1)  # Ensure values are valid\n",
    "    return image_tensor\n",
    "\n",
    "#############################################\n",
    "#             Segmentation Head             #\n",
    "#############################################\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class ImprovedCLIPSegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels, text_dim, num_classes, dropout_prob=0.25, use_attention=True):\n",
    "        super(ImprovedCLIPSegmentationHead, self).__init__()\n",
    "        # Project text features to match visual feature channels.\n",
    "        self.text_proj = nn.Linear(text_dim, in_channels)\n",
    "        self.fuse_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(dropout_prob)\n",
    "        )\n",
    "        self.residual_block = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.attention = ChannelAttention(256) if use_attention else nn.Identity()\n",
    "        self.fuse_conv2 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, img_features, text_features):\n",
    "        B, C, H, W = img_features.shape  # Expected C = 768\n",
    "        img_features = img_features.float()\n",
    "        text_features = text_features.float()\n",
    "\n",
    "        projected_text = self.text_proj(text_features)  # [B, 768]\n",
    "        projected_text = projected_text.view(B, C, 1, 1).expand(B, C, H, W)\n",
    "        fused = torch.cat([img_features, projected_text], dim=1)\n",
    "        x = self.fuse_conv1(fused)\n",
    "        res = self.residual_block(x)\n",
    "        x = self.relu(x + res)\n",
    "        x = self.attention(x)\n",
    "        logits = self.fuse_conv2(x)\n",
    "        return logits\n",
    "\n",
    "class CLIPSegmentationModel(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=3):\n",
    "        super(CLIPSegmentationModel, self).__init__()\n",
    "        # Get feature dimension from the CLIP visual model (e.g., 768).\n",
    "        self.feature_dim = clip_model.visual.output_dim\n",
    "        print(f'Feature dimension: {self.feature_dim}')  # Expected 768\n",
    "        \n",
    "        self.clip_model = clip_model\n",
    "        # Segmentation head with input channels and text dimension matching CLIP.\n",
    "        self.seg_head = ImprovedCLIPSegmentationHead(\n",
    "            in_channels=self.feature_dim,\n",
    "            text_dim=self.feature_dim,\n",
    "            num_classes=num_classes,\n",
    "            dropout_prob=0.25\n",
    "        )\n",
    "    \n",
    "    def get_visual_features(self, image):\n",
    "        visual = self.clip_model.visual\n",
    "        image = image.to(dtype=visual.conv1.weight.dtype)\n",
    "        x = visual.conv1(image)  # [B, width, H', W']\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1).permute(0, 2, 1)  # [B, tokens, width]\n",
    "        cls_tokens = visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = x + visual.positional_embedding.to(x.dtype)\n",
    "        x = visual.ln_pre(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = visual.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        if hasattr(visual, \"proj\"):\n",
    "            x = x @ visual.proj\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, token_ids):\n",
    "      image = image.float()\n",
    "      # Compute CLIP features without tracking gradients.\n",
    "      with torch.no_grad():\n",
    "          visual_features = self.get_visual_features(image)  # [B, num_tokens+1, 768]\n",
    "          tokens = visual_features[:, 1:, :]  # remove the class token\n",
    "          B, N, D = tokens.shape\n",
    "          grid_size = int(np.sqrt(N))\n",
    "          image_features = tokens.reshape(B, grid_size, grid_size, D).permute(0, 3, 1, 2)\n",
    "          text_features = self.clip_model.encode_text(token_ids).float()\n",
    "          \n",
    "      # Compute segmentation head normally (gradients tracked here).\n",
    "      seg_logits = self.seg_head(image_features, text_features)\n",
    "      seg_logits = F.interpolate(seg_logits, size=(image.shape[2], image.shape[3]),\n",
    "                                mode=\"bilinear\", align_corners=False)\n",
    "      return seg_logits\n",
    "\n",
    "###############################################\n",
    "# Training, Evaluation, and Utility Functions #\n",
    "###############################################\n",
    "def compute_iou_per_class(preds, targets, num_classes=3):\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union (IoU) for each class.\n",
    "    Returns a dictionary with IoU for each class.\n",
    "    \"\"\"\n",
    "    iou_per_class = {}\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (preds == cls)\n",
    "        target_inds = (targets == cls)\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = (pred_inds | target_inds).sum().item()\n",
    "\n",
    "        if union == 0:\n",
    "            iou_per_class[cls] = float('nan')  # Avoid division by zero\n",
    "        else:\n",
    "            iou_per_class[cls] = intersection / union\n",
    "\n",
    "    return iou_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the robustness of our CLIP features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimension: 768\n",
      "Perturbation level 0 (gaussian_noise): Mean Dice Score = 0.9388\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 222\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Specify the path to the saved model.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip_OPENAI_segmentation_best.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 222\u001b[0m dice_scores \u001b[38;5;241m=\u001b[39m test_model_on_perturbations(model_path, test_dataset, perturbation_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgaussian_noise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    223\u001b[0m dice_scores \u001b[38;5;241m=\u001b[39m test_model_on_perturbations(model_path, test_dataset, perturbation_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgaussian_blur\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    224\u001b[0m dice_scores \u001b[38;5;241m=\u001b[39m test_model_on_perturbations(model_path, test_dataset, perturbation_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrast_increase\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 182\u001b[0m, in \u001b[0;36mtest_model_on_perturbations\u001b[0;34m(model_path, test_dataset, batch_size, num_classes, perturbation_mode)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Run through model.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 182\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(perturbed_tensor, token_id)\n\u001b[1;32m    183\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Compute Dice ignoring boundaries.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 235\u001b[0m, in \u001b[0;36mCLIPSegmentationModel.forward\u001b[0;34m(self, image, token_ids)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Compute CLIP features without tracking gradients.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 235\u001b[0m     visual_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_visual_features(image)  \u001b[38;5;66;03m# [B, num_tokens+1, 768]\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m visual_features[:, \u001b[38;5;241m1\u001b[39m:, :]  \u001b[38;5;66;03m# remove the class token\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     B, N, D \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[1], line 225\u001b[0m, in \u001b[0;36mCLIPSegmentationModel.get_visual_features\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    223\u001b[0m x \u001b[38;5;241m=\u001b[39m visual\u001b[38;5;241m.\u001b[39mln_pre(x)\n\u001b[1;32m    224\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 225\u001b[0m x \u001b[38;5;241m=\u001b[39m visual\u001b[38;5;241m.\u001b[39mtransformer(x)\n\u001b[1;32m    226\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(visual, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproj\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/clip/model.py:203\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresblocks(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/clip/model.py:190\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 190\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x))\n\u001b[1;32m    191\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/clip/model.py:187\u001b[0m, in \u001b[0;36mResidualAttentionBlock.attention\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattention\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x, x, x, need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/modules/activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1343\u001b[0m         query,\n\u001b[1;32m   1344\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1369\u001b[0m         query,\n\u001b[1;32m   1370\u001b[0m         key,\n\u001b[1;32m   1371\u001b[0m         value,\n\u001b[1;32m   1372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[1;32m   1373\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1374\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight,\n\u001b[1;32m   1375\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k,\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v,\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1382\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1383\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1384\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1385\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1386\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1387\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1388\u001b[0m     )\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/anaconda3/envs/recipemag/lib/python3.12/site-packages/torch/nn/functional.py:6386\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6383\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   6384\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 6386\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(\n\u001b[1;32m   6387\u001b[0m     q, k, v, attn_mask, dropout_p, is_causal\n\u001b[1;32m   6388\u001b[0m )\n\u001b[1;32m   6389\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   6390\u001b[0m     attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   6391\u001b[0m )\n\u001b[1;32m   6393\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from skimage.util import random_noise\n",
    "\n",
    "# --- Provided perturbation functions (unchanged) ---\n",
    "def add_salt_and_pepper_noise(image, amount):\n",
    "    image_float = image.astype(np.float32) / 255.0\n",
    "    noisy = random_noise(image_float, mode='s&p', amount=amount)\n",
    "    noisy = np.clip(noisy * 255.0, 0, 255).astype(np.uint8)\n",
    "    return noisy\n",
    "\n",
    "def occlude_image(image, square_edge):\n",
    "    perturbed = image.copy()\n",
    "    if square_edge > 0:\n",
    "        h, w, _ = perturbed.shape\n",
    "        max_x = w - square_edge\n",
    "        max_y = h - square_edge\n",
    "        if max_x < 0 or max_y < 0:\n",
    "            perturbed[:] = 0\n",
    "        else:\n",
    "            x = np.random.randint(0, max_x + 1)\n",
    "            y = np.random.randint(0, max_y + 1)\n",
    "            perturbed[y:y+square_edge, x:x+square_edge, :] = 0\n",
    "    return perturbed\n",
    "\n",
    "def decrease_brightness(image, offset):\n",
    "    perturbed = image.astype(np.int32) - offset\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def increase_brightness(image, offset):\n",
    "    perturbed = image.astype(np.int32) + offset\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def decrease_contrast(image, factor):\n",
    "    perturbed = image.astype(np.float32) * factor\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def increase_contrast(image, factor):\n",
    "    perturbed = image.astype(np.float32) * factor\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def add_gaussian_noise(image, sigma):\n",
    "    noise = np.random.normal(0, sigma, image.shape)\n",
    "    perturbed = image.astype(np.float32) + noise\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def add_gaussian_blur(image, iterations):\n",
    "    import cv2\n",
    "    kernel = (1/16) * np.array([[1, 2, 1],\n",
    "                                [2, 4, 2],\n",
    "                                [1, 2, 1]], dtype=np.float32)\n",
    "    perturbed = image.copy()\n",
    "    for _ in range(iterations):\n",
    "        perturbed = cv2.filter2D(perturbed, ddepth=-1, kernel=kernel)\n",
    "    return perturbed\n",
    "\n",
    "# --- Updated Perturbation Evaluation Function ---\n",
    "def test_model_on_perturbations(model_path, test_dataset, batch_size=1, num_classes=3, perturbation_mode='gaussian_noise'):\n",
    "    \"\"\"\n",
    "    Evaluates the CLIP features segmentation model on perturbed test data.\n",
    "    \n",
    "    For each perturbation level, each test sample is:\n",
    "      1. Denormalized to [0,255] (uint8),\n",
    "      2. Perturbed using the specified function,\n",
    "      3. Re-normalized with CLIP’s mean and std,\n",
    "      4. Passed through the segmentation model (with its text tokens),\n",
    "      5. Compared with the ground truth after setting all boundary pixels (i.e. pixels not 0,1,2) to ignore (255).\n",
    "    \n",
    "    The function computes the mean Dice score (ignoring boundaries) at each perturbation level,\n",
    "    plots mean Dice versus perturbation level, and returns the list of mean Dice scores.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # CLIP normalization parameters.\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3,1,1).to(device)\n",
    "    std  = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3,1,1).to(device)\n",
    "    \n",
    "    # Load CLIP model.\n",
    "    clip_model, _ = clip.load(\"ViT-L/14\", device=device)\n",
    "    clip_model.eval()\n",
    "    for param in clip_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Load your CLIP segmentation model.\n",
    "    model = CLIPSegmentationModel(clip_model=clip_model, num_classes=num_classes).to(device)\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Set up perturbation levels and function.\n",
    "    if perturbation_mode == 'gaussian_noise':\n",
    "        levels = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
    "        perturb_fn = add_gaussian_noise\n",
    "    elif perturbation_mode == 'gaussian_blur':\n",
    "        levels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        perturb_fn = add_gaussian_blur\n",
    "    elif perturbation_mode == 'contrast_increase':\n",
    "        levels = [1.0, 1.01, 1.02, 1.03, 1.04, 1.05, 1.1, 1.15, 1.2, 1.25]\n",
    "        perturb_fn = increase_contrast\n",
    "    elif perturbation_mode == 'contrast_decrease':\n",
    "        levels = [1.0, 0.95, 0.90, 0.85, 0.80, 0.60, 0.40, 0.30, 0.20, 0.10]\n",
    "        perturb_fn = decrease_contrast\n",
    "    elif perturbation_mode == 'brightness_increase':\n",
    "        levels = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "        perturb_fn = increase_brightness\n",
    "    elif perturbation_mode == 'brightness_decrease':\n",
    "        levels = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "        perturb_fn = decrease_brightness\n",
    "    elif perturbation_mode == 'occlusion_increase':\n",
    "        levels = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "        perturb_fn = occlude_image\n",
    "    elif perturbation_mode == 'salt_and_pepper_noise':\n",
    "        levels = [0.00, 0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.14, 0.16, 0.18]\n",
    "        perturb_fn = add_salt_and_pepper_noise\n",
    "    else:\n",
    "        raise ValueError(f\"Perturbation mode '{perturbation_mode}' not implemented.\")\n",
    "    \n",
    "    dice_scores_per_level = []\n",
    "    \n",
    "    # Define a helper function to compute Dice ignoring boundary pixels.\n",
    "    def dice_ignore(pred, target, num_classes=3, epsilon=1e-6):\n",
    "        # target: 2D tensor; set any pixel not in {0,1,2} to ignore (255)\n",
    "        target_metric = target.clone()\n",
    "        target_metric[~((target_metric == 0) | (target_metric == 1) | (target_metric == 2))] = 255\n",
    "        valid = (target_metric != 255)\n",
    "        if valid.sum() == 0:\n",
    "            return float('nan')\n",
    "        dice_per_class = []\n",
    "        for cls in range(num_classes):\n",
    "            pred_cls = (pred[valid] == cls).float()\n",
    "            target_cls = (target_metric[valid] == cls).float()\n",
    "            intersection = (pred_cls * target_cls).sum()\n",
    "            union = pred_cls.sum() + target_cls.sum()\n",
    "            if union == 0:\n",
    "                dice_per_class.append(float('nan'))\n",
    "            else:\n",
    "                dice_per_class.append((2 * intersection + epsilon) / (union + epsilon))\n",
    "        return np.nanmean(dice_per_class)\n",
    "    \n",
    "    # Loop over each perturbation level.\n",
    "    for level in levels:\n",
    "        level_dice_scores = []\n",
    "        # Process each sample in the test set.\n",
    "        for images, masks, token_ids in test_loader:\n",
    "            # Process each sample in the batch.\n",
    "            for i in range(images.size(0)):\n",
    "                # Extract a single sample.\n",
    "                image = images[i]         # (C, H, W)\n",
    "                mask = masks[i]           # (H, W)\n",
    "                token_id = token_ids[i]   # token IDs tensor\n",
    "                \n",
    "                # Denormalize image to [0,255] (uint8).\n",
    "                denorm_img = denormalize(image)\n",
    "                np_img = (denorm_img * 255.0).clamp(0, 255).cpu().numpy().transpose(1,2,0).astype(np.uint8)\n",
    "                \n",
    "                # Apply perturbation.\n",
    "                perturbed_np = perturb_fn(np_img, level)\n",
    "                \n",
    "                # Convert back to tensor and re-normalize.\n",
    "                perturbed_tensor = torch.from_numpy(perturbed_np).permute(2, 0, 1).float() / 255.0\n",
    "                perturbed_tensor = perturbed_tensor.to(device)  # move to the same device as mean and std\n",
    "                perturbed_tensor = (perturbed_tensor - mean) / std\n",
    "                perturbed_tensor = perturbed_tensor.unsqueeze(0)  \n",
    "                \n",
    "                # Process token IDs and mask for this sample.\n",
    "                # Process token IDs and mask for this sample.\n",
    "                token_id = token_id.unsqueeze(0).to(device)\n",
    "                mask = mask.unsqueeze(0).to(device).cpu()  # move mask to CPU\n",
    "                # Run through model.\n",
    "                with torch.no_grad():\n",
    "                    output = model(perturbed_tensor, token_id)\n",
    "                pred = torch.argmax(output, dim=1).squeeze(0).cpu()\n",
    "\n",
    "                # Now both pred and mask are on CPU.\n",
    "                dice_val = dice_ignore(pred, mask.squeeze(0), num_classes=num_classes)\n",
    "                level_dice_scores.append(dice_val)\n",
    "        \n",
    "        mean_dice = np.nanmean(level_dice_scores)\n",
    "        dice_scores_per_level.append(mean_dice)\n",
    "        print(f\"Perturbation level {level} ({perturbation_mode}): Mean Dice Score = {mean_dice:.4f}\")\n",
    "    \n",
    "    # Plot mean Dice vs. perturbation level.\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(levels, dice_scores_per_level, marker='o')\n",
    "    plt.xlabel(f\"{perturbation_mode} level\")\n",
    "    plt.ylabel(\"Mean Dice Score\")\n",
    "    plt.title(f\"Robustness Evaluation: Dice Score vs {perturbation_mode}\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{perturbation_mode}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    return dice_scores_per_level\n",
    "\n",
    "# Set device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {device} device')\n",
    "# Load CLIP's preprocessing transform.\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "test_transform = clip_preprocess\n",
    "\n",
    "# Create your test dataset (adjust the root_dir path as needed).\n",
    "test_dataset = SegmentationDatasetWithText(\n",
    "    root_dir=\"./Dataset/processed/Test\",\n",
    "    transform_img=test_transform,\n",
    "    transform_label=T.Resize((224, 224), interpolation=T.InterpolationMode.NEAREST)\n",
    ")\n",
    "\n",
    "# Specify the path to the saved model.\n",
    "model_path = \"clip_OPENAI_segmentation_best.pth\"\n",
    "\n",
    "dice_scores = test_model_on_perturbations(model_path, test_dataset, perturbation_mode='gaussian_noise')\n",
    "dice_scores = test_model_on_perturbations(model_path, test_dataset, perturbation_mode='gaussian_blur')\n",
    "dice_scores = test_model_on_perturbations(model_path, test_dataset, perturbation_mode='contrast_increase')\n",
    "dice_scores = test_model_on_perturbations(model_path, test_dataset, perturbation_mode='contrast_decrease')\n",
    "dice_scores = test_model_on_perturbations(model_path, test_dataset, perturbation_mode='brightness_increase')\n",
    "dice_scores = test_model_on_perturbations(model_path, test_dataset, perturbation_mode='brightness_decrease')\n",
    "dice_scores = test_model_on_perturbations(model_path, test_dataset, perturbation_mode='occlusion_increase')\n",
    "dice_scores = test_model_on_perturbations(model_path, test_dataset, perturbation_mode='salt_and_pepper_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import random_noise\n",
    "\n",
    "def add_salt_and_pepper_noise(image, amount):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    amount: float value representing the proportion of pixels to be replaced with noise.\n",
    "            For example: 0.00, 0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.14, 0.16, 0.18.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) with salt and pepper noise added.\n",
    "    \"\"\"\n",
    "    # Convert image to float in range [0,1] for skimage\n",
    "    image_float = image.astype(np.float32) / 255.0\n",
    "    noisy = random_noise(image_float, mode='s&p', amount=amount)\n",
    "    # Convert back to [0,255] uint8\n",
    "    noisy = np.clip(noisy * 255.0, 0, 255).astype(np.uint8)\n",
    "    return noisy\n",
    "\n",
    "def occlude_image(image, square_edge):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    square_edge: integer, the edge length of the square to occlude.\n",
    "                 For example: 0, 5, 10, 15, 20, 25, 30, 35, 40, 45.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) with a randomly placed square region occluded.\n",
    "    \"\"\"\n",
    "    perturbed = image.copy()\n",
    "    if square_edge > 0:\n",
    "        h, w, _ = perturbed.shape\n",
    "        # Ensure the square fits within the image\n",
    "        max_x = w - square_edge\n",
    "        max_y = h - square_edge\n",
    "        if max_x < 0 or max_y < 0:\n",
    "            # If the square is larger than the image, occlude the whole image\n",
    "            perturbed[:] = 0\n",
    "        else:\n",
    "            # Randomly select the top-left corner for occlusion.\n",
    "            x = np.random.randint(0, max_x + 1)\n",
    "            y = np.random.randint(0, max_y + 1)\n",
    "            perturbed[y:y+square_edge, x:x+square_edge, :] = 0\n",
    "    return perturbed\n",
    "\n",
    "def decrease_brightness(image, offset):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    offset: integer offset to subtract from each pixel. For example: 0, 5, 10, 15, 20, 25, 30, 35, 40, 45.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) with decreased brightness.\n",
    "    \"\"\"\n",
    "    # Convert to a type that supports negative values.\n",
    "    perturbed = image.astype(np.int32) - offset\n",
    "    # Clip values so that they do not fall below 0.\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def increase_brightness(image, offset):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    offset: integer offset to add to each pixel. For example: 0, 5, 10, 15, 20, 25, 30, 35, 40, 45.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) with increased brightness.\n",
    "    \"\"\"\n",
    "    # Convert to an integer type that can hold values > 255\n",
    "    perturbed = image.astype(np.int32) + offset\n",
    "    # Clip values to the valid range and convert back to uint8\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def decrease_contrast(image, factor):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    factor: multiplicative factor for pixel values.\n",
    "            For example: 1.0, 0.95, 0.90, 0.85, 0.80, 0.60, 0.40, 0.30, 0.20, 0.10.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) with decreased contrast.\n",
    "    \"\"\"\n",
    "    perturbed = image.astype(np.float32) * factor\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def increase_contrast(image, factor):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    factor: multiplicative factor for pixel values.\n",
    "            For example: 1.0, 1.01, 1.02, 1.03, 1.04, 1.05, 1.1, 1.15, 1.2, 1.25.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) with increased contrast.\n",
    "    \"\"\"\n",
    "    perturbed = image.astype(np.float32) * factor\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def add_gaussian_noise(image, sigma):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    sigma: standard deviation of the Gaussian noise.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255)\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, sigma, image.shape)\n",
    "    perturbed = image.astype(np.float32) + noise\n",
    "    perturbed = np.clip(perturbed, 0, 255).astype(np.uint8)\n",
    "    return perturbed\n",
    "\n",
    "def add_gaussian_blur(image, iterations):\n",
    "    \"\"\"\n",
    "    image: numpy array of shape (H, W, C) with dtype=np.uint8, values in 0-255.\n",
    "    iterations: integer, the number of times to convolve the image with a 3x3 Gaussian kernel.\n",
    "                Use 0 for no blurring, 1 for a single pass, up to 9 for a heavy blur.\n",
    "    Returns:\n",
    "        perturbed_image: numpy array (uint8, values in 0-255) that has been blurred.\n",
    "    \"\"\"\n",
    "    import cv2  # OpenCV is used for the convolution.\n",
    "    # Define a 3x3 Gaussian kernel.\n",
    "    kernel = (1/16) * np.array([[1, 2, 1],\n",
    "                                [2, 4, 2],\n",
    "                                [1, 2, 1]], dtype=np.float32)\n",
    "    \n",
    "    perturbed = image.copy()\n",
    "    for _ in range(iterations):\n",
    "        perturbed = cv2.filter2D(perturbed, ddepth=-1, kernel=kernel)\n",
    "    return perturbed\n",
    "\n",
    "\n",
    "def dice_score_multiclass(pred, target, num_classes=3, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the mean Dice score over num_classes.\n",
    "    pred and target should be 2D tensors of shape [H, W] containing class indices.\n",
    "    \"\"\"\n",
    "    dice_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls).float()\n",
    "        target_cls = (target == cls).float()\n",
    "        intersection = (pred_cls * target_cls).sum()\n",
    "        union = pred_cls.sum() + target_cls.sum()\n",
    "        dice_cls = (2 * intersection + epsilon) / (union + epsilon)\n",
    "        dice_per_class.append(dice_cls.item())\n",
    "    return np.mean(dice_per_class)\n",
    "\n",
    "def test_best_model_on_perturbations(best_model_path, test_dataset, batch_size=1, num_classes=3, perturbation_mode='gaussian_noise'):\n",
    "    \"\"\"\n",
    "    Evaluates the best segmentation model on perturbed test data.\n",
    "    For each perturbation level, the function:\n",
    "      - Denormalizes the input image to [0,255]\n",
    "      - Applies the chosen perturbation\n",
    "      - Re-normalizes the image using CLIP's mean/std\n",
    "      - Feeds the perturbed image (with its token_ids) into the model\n",
    "      - Computes the multi-class Dice score over the test set.\n",
    "    Plots the mean Dice score versus perturbation level.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Load CLIP model and set to evaluation.\n",
    "    clip_model, _ = clip.load(\"ViT-L/14\", device=device)\n",
    "    clip_model.eval()\n",
    "    for param in clip_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Load your CLIP segmentation model.\n",
    "    model = CLIPSegmentationModel(clip_model=clip_model, num_classes=num_classes).to(device)\n",
    "    state_dict = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    # Define CLIP normalization parameters.\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
    "\n",
    "    # Set up perturbation parameters.\n",
    "    if perturbation_mode == 'gaussian_noise':\n",
    "        levels = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
    "        perturb_fn = add_gaussian_noise\n",
    "    elif perturbation_mode == 'gaussian_blur':\n",
    "        levels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        perturb_fn = add_gaussian_blur\n",
    "    elif perturbation_mode == 'contrast_increase':\n",
    "        levels = [1.0, 1.01, 1.02, 1.03, 1.04, 1.05, 1.1, 1.15, 1.2, 1.25]\n",
    "        perturb_fn = increase_contrast\n",
    "    elif perturbation_mode == 'contrast_decrease':\n",
    "        levels = [1.0, 0.95, 0.90, 0.85, 0.80, 0.60, 0.40, 0.30, 0.20, 0.10]\n",
    "        perturb_fn = decrease_contrast\n",
    "    elif perturbation_mode == 'brightness_increase':\n",
    "        levels = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "        perturb_fn = increase_brightness\n",
    "    elif perturbation_mode == 'brightness_decrease':\n",
    "        levels = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "        perturb_fn = decrease_brightness\n",
    "    elif perturbation_mode == 'occlusion_increase':\n",
    "        levels = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "        perturb_fn = occlude_image\n",
    "    elif perturbation_mode == 'salt_and_pepper_noise':\n",
    "        levels = [0.00, 0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.14, 0.16, 0.18]\n",
    "        perturb_fn = add_salt_and_pepper_noise\n",
    "    else:\n",
    "        raise ValueError(f\"Perturbation mode '{perturbation_mode}' not implemented.\")\n",
    "\n",
    "    dice_scores_per_level = []\n",
    "\n",
    "    # Loop over each perturbation level.\n",
    "    for level in levels:\n",
    "        level_dice_scores = []\n",
    "        for images, masks, token_ids in test_loader:\n",
    "            # Denormalize the image to [0,1] then scale to [0,255].\n",
    "            images_denorm = denormalize(images)\n",
    "            images_pixels = (images_denorm * 255.0).clamp(0, 255).cpu().numpy().astype(np.uint8)\n",
    "            # Process first image in batch.\n",
    "            img_np = np.transpose(images_pixels[0], (1, 2, 0))\n",
    "            \n",
    "            # Apply the perturbation.\n",
    "            perturbed_np = perturb_fn(img_np, level)\n",
    "            \n",
    "            # Convert perturbed image back to a tensor.\n",
    "            perturbed_tensor = torch.from_numpy(perturbed_np).permute(2, 0, 1).unsqueeze(0).float()\n",
    "            perturbed_tensor = perturbed_tensor / 255.0\n",
    "            perturbed_tensor = (perturbed_tensor - mean) / std\n",
    "            perturbed_tensor = perturbed_tensor.to(device)\n",
    "\n",
    "            token_ids = token_ids.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(perturbed_tensor, token_ids)\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze(0)\n",
    "\n",
    "            # Compute multi-class Dice score.\n",
    "            dice = dice_score_multiclass(pred_mask, masks, num_classes=num_classes)\n",
    "            level_dice_scores.append(dice)\n",
    "        mean_dice = np.mean(level_dice_scores)\n",
    "        dice_scores_per_level.append(mean_dice)\n",
    "        print(f\"Perturbation Level {level} ({perturbation_mode}): Mean Dice Score = {mean_dice:.4f}\")\n",
    "\n",
    "    # Plot mean Dice score vs. perturbation level.\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(levels, dice_scores_per_level, marker='o')\n",
    "    plt.xlabel(f\"{perturbation_mode} level\")\n",
    "    plt.ylabel(\"Mean Dice Score\")\n",
    "    plt.title(f\"Robustness Evaluation: Dice Score vs {perturbation_mode}\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{perturbation_mode}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    return dice_scores_per_level\n",
    "\n",
    "###############################################\n",
    "#         Example Usage in __main__          #\n",
    "###############################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load CLIP's preprocessing transform.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    clip_model, clip_preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "    test_transform = clip_preprocess  # use CLIP's preprocessing\n",
    "\n",
    "    # For labels, we now simply resize the mask to 224x224 (no extra encoding needed)\n",
    "    transform_label = T.Resize((224, 224), interpolation=T.InterpolationMode.NEAREST)\n",
    "\n",
    "    # Create your test dataset (adjust root_dir as needed).\n",
    "    test_dataset = SegmentationDatasetWithText(\n",
    "        root_dir=\"./processed/Test/\",\n",
    "        transform_img=test_transform,\n",
    "        transform_label=transform_label\n",
    "    )\n",
    "\n",
    "    best_model_path = \"./clip_OPENAI_segmentation_best.pth\"\n",
    "\n",
    "    # Evaluate different perturbation modes.\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='gaussian_noise')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='gaussian_blur')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='contrast_increase')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='contrast_decrease')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='brightness_increase')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='brightness_decrease')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='occlusion_increase')\n",
    "    dice_scores = test_best_model_on_perturbations(best_model_path, test_dataset, perturbation_mode='salt_and_pepper_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged PDF saved as merged_grid.pdf\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "\n",
    "def merge_pdfs_to_grid(pdf_list, output_path, grid=(2, 4), dpi=150):\n",
    "    \"\"\"\n",
    "    Render the first page of each PDF as an image, arrange them in a grid,\n",
    "    and save the result as a single-page PDF.\n",
    "    \n",
    "    Arguments:\n",
    "      pdf_list: list of PDF file paths (should have 8 PDFs for a 2x4 grid)\n",
    "      output_path: output PDF file path.\n",
    "      grid: tuple (rows, columns) for the layout; default is (2, 4).\n",
    "      dpi: rendering resolution.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for pdf in pdf_list:\n",
    "        doc = fitz.open(pdf)\n",
    "        page = doc.load_page(0)\n",
    "        # Compute zoom factor (default resolution is 72 dpi)\n",
    "        zoom = dpi / 72.0\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        # Convert pixmap to a PIL Image (RGB)\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        images.append(img)\n",
    "        doc.close()\n",
    "    \n",
    "    # Determine grid dimensions.\n",
    "    rows, cols = grid\n",
    "    if len(images) != rows * cols:\n",
    "        raise ValueError(f\"Expected {rows*cols} images, but got {len(images)}.\")\n",
    "\n",
    "    # Assume all images have similar sizes; use the maximum width and height.\n",
    "    widths, heights = zip(*(im.size for im in images))\n",
    "    max_width = max(widths)\n",
    "    max_height = max(heights)\n",
    "    \n",
    "    # Create a new blank image with white background.\n",
    "    merged_width = cols * max_width\n",
    "    merged_height = rows * max_height\n",
    "    merged_img = Image.new('RGB', (merged_width, merged_height), color=(255, 255, 255))\n",
    "    \n",
    "    # Paste each image into its grid cell.\n",
    "    for idx, im in enumerate(images):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        x_offset = col * max_width\n",
    "        y_offset = row * max_height\n",
    "        merged_img.paste(im, (x_offset, y_offset))\n",
    "    \n",
    "    # Save the merged image as a single-page PDF.\n",
    "    merged_img.save(output_path, \"PDF\", resolution=dpi)\n",
    "    print(f\"Merged PDF saved as {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List your 8 PDF figure files here.\n",
    "    pdf_files = [\n",
    "        'brightness_decrease.pdf', 'brightness_increase.pdf', 'contrast_decrease.pdf', 'contrast_increase.pdf',\n",
    "        'gaussian_blur.pdf', 'gaussian_noise.pdf', 'occlusion_increase.pdf', 'salt_and_pepper_noise.pdf'\n",
    "    ]\n",
    "    output_pdf = 'merged_grid.pdf'\n",
    "    merge_pdfs_to_grid(pdf_files, output_pdf, grid=(2, 4), dpi=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipemag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
